
[uwsgi]
module = main:app
master = true

processes = 4 #single process. parallel instances of app. isolated childs, don't share resources. maybe 8 (logical cores) or 4 (cores/2 instances)
threads = 2 #number of parallel requests each child(process) can handle. maybe 2 (each core has 2 threads)
#workers = 4
cheaper-algo = busyness
cheaper = 1
cheaper-initial = 2
cheaper-overload = 5
cheaper-step = 1
cheaper-busyness-multiplier = 30     ; How many cycles to wait before killing workers
#cheaper-busyness-min = 20            ; Below this threshold, kill workers (if stable for multiplier cycles)
#cheaper-busyness-max = 70            ; Above this threshold, spawn new workers
cheaper-busyness-backlog-alert = 100 ; Spawn emergency workers if more than this many requests are waiting in the queue
cheaper-busyness-backlog-step = 2    ; How many emergegency workers to create if there are too many requests in the queue

enable-threads = true
single-interpreter = true #false here allows multiple services to be hosted in each worker process, but we don't need it and can cause problems let true
thunder-lock = true
lazy-apps = true #true to load app separately for every worker. helps avoiding errors when sharing data between workers
listen = 1024 #1024 #set this parameter to the maximum number of unique users you expect to enqueue during a reload. Linux has 128 at max by default. increase your kernel limit too. /proc/sys/net/core/somaxconn

harakiri = 60
#buffer-size = 32768
max-requests = 3500 #or 20480 ; Restart workers after this many requests
max-worker-lifetime = 3600 ; Restart workers after this many seconds
#reload-on-rss = 2048 ; Restart workers after this much resident memory
worker-reload-mercy = 60 #60 ; How long to wait before forcefully killing workers
need-app = true
socket = 0.0.0.0:5000
chmod-socket = 660
vacuum = true
die-on-term = true
strict = true
disable-logging = true
log-4xx = true
log-5xx = true
py-call-osafterfork = true
auto-procname = true